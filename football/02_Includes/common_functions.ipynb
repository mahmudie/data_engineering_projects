{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5b4e10-113c-4479-a269-32b18cb721d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da66071-fca2-49a8-a1fd-fcee643320d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/mnt/databricksdl2/footballprocessed'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%run \"../02_Includes/configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472ab120-cd09-40e4-bc88-473f0b377420",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# **function to fill time stamp the data set were ingested, data source, and table name columns**\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "def create_new_column(input_df, file_name, source_name):\n",
    "    output_df=input_df.withColumn(\"ingestion_date\",current_timestamp())\\\n",
    "        .withColumn(\"data_source_name\",lit(source_name))\\\n",
    "        .withColumn(\"file_name\",lit(file_name))\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d702c89-d598-42c7-9c8d-8e8836a69268",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# **This function helps in converting string to date format**\n",
    "from datetime import datetime\n",
    "def convert_to_date(input_string):\n",
    "    year = int(input_string[0:4])\n",
    "    month =int(input_string[5:7])\n",
    "    day=int(input_string[8:])\n",
    "    date_c=datetime(year, month, day)\n",
    "    return date_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1bc69a-4559-436f-a93f-ce2d5a5a0e78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# **Function to ingest data from the source given as parameter and dynamically save the Spark Data Frame into CSV for further proccessig**\n",
    "def create_table(source_path,destination_path,table_name,file_format_source,file_format_destination,data_source_name,df_type):\n",
    "    import pandas as pd\n",
    "    data_source_url = f\"{source_path}/{table_name}.{file_format_source}\" \n",
    "    # Read data from the source using Pandas\n",
    "    extracted_python_df = pd.read_csv(data_source_url)\n",
    "    extracted_python_df['continentid']=extracted_python_df['home_continent'].str[:2].apply(lambda x:x.upper())+extracted_python_df['away_continent'].str[:2].apply(lambda x:x.upper())+extracted_python_df['continent'].str[:2].apply(lambda x:x.upper())\n",
    "\n",
    "    extracted_python_df['competitionid']=extracted_python_df['away_country'].str[:4].apply(lambda x:x.upper())+extracted_python_df['home_country'].str[:4].apply(lambda x:x.upper())+extracted_python_df['competition'].str[:4].apply(lambda x:x.upper())+'-'+extracted_python_df['home_code']+\"-\" +extracted_python_df['away_code']\n",
    "\n",
    "    # Read data from Pandas data from and convert schema\n",
    "    extracted_spark_df = spark.createDataFrame(extracted_python_df.astype(df_type))\n",
    "    # Add new columns to the Spark Data Frame\n",
    "    extracted_spark_renamed_df=create_new_column(extracted_spark_df,table_name, data_source_name)\n",
    "\n",
    "    # Save spark data frame into ADLS storage account\n",
    "    extracted_spark_renamed_df.write.format(file_format_destination).mode(\"overwrite\").save(f\"{footballingested_folder_path}/{table_name}\")\n",
    "    # display(extracted_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90ce909-c5ae-4f0b-b510-3e75c6b57075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def movepartition (df,partitionId):\n",
    "    df_columns =df.schema.names\n",
    "    final_df=df\n",
    "    if partitionId !=\"0\" and partitionId in df_columns:\n",
    "        df_columns.remove(partitionId)\n",
    "        df_columns.append(partitionId)\n",
    "        final_df =df.select(df_columns)\n",
    "    else:\n",
    "        final_df =df\n",
    "        \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4d9630-68d4-43f8-9d61-afa1d3222421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_delta_data(df,database_name,table_name, folder_path,merge_condition,partitionId):\n",
    "    from delta.tables import DeltaTable\n",
    "    if(spark._jsparkSession.catalog().tableExists(f\"{database_name}.{table_name}\")):\n",
    "        deltaTable = DeltaTable.forPath(spark,f\"{folder_path}/{table_name}\")\n",
    "        deltaTable.alias(\"tgt\").merge (\n",
    "            df.alias(\"src\"),merge_condition\n",
    "        )\\\n",
    "            .whenMatchedUpdateAll()\\\n",
    "            .whenNotMatchedInsertAll()\\\n",
    "            .execute()\n",
    "    else:\n",
    "        df.write.mode(\"overwrite\").partitionBy(partitionId).format(\"delta\").saveAsTable(f\"{database_name}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
